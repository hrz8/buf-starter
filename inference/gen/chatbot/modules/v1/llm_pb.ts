// @generated by protoc-gen-es v2.6.3 with parameter "target=ts,import_extension=js"
// @generated from file chatbot/modules/v1/llm.proto (package chatbot.modules.v1, syntax proto3)
/* eslint-disable */

import type { GenFile, GenMessage } from "@bufbuild/protobuf/codegenv2";
import { fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv2";
import { file_buf_validate_validate } from "../../../buf/validate/validate_pb.js";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file chatbot/modules/v1/llm.proto.
 */
export const file_chatbot_modules_v1_llm: GenFile = /*@__PURE__*/
  fileDesc("ChxjaGF0Ym90L21vZHVsZXMvdjEvbGxtLnByb3RvEhJjaGF0Ym90Lm1vZHVsZXMudjEiwwEKCUxsbUNvbmZpZxIPCgdlbmFibGVkGAEgASgIEhwKA3NkaxgCIAEoCUIPukgMcgpSBmFpLXNka1IAEiIKCHByb3ZpZGVyGAMgASgJQhC6SA1yC1IHYmVkcm9ja1IAEhcKBW1vZGVsGAQgASgJQgi6SAVyAxjIARIsCgt0ZW1wZXJhdHVyZRgFIAEoAUIXukgUEhIZAAAAAAAAAEApAAAAAAAAAAASHAoJbWF4X3N0ZXBzGAYgASgFQgm6SAYaBBgyKAFCzgEKFmNvbS5jaGF0Ym90Lm1vZHVsZXMudjFCCExsbVByb3RvUAFaQGdpdGh1Yi5jb20vaHJ6OC9hbHRhbHVuZS9nZW4vY2hhdGJvdC9tb2R1bGVzL3YxO2NoYXRib3Rtb2R1bGVzdjGiAgNDTViqAhJDaGF0Ym90Lk1vZHVsZXMuVjHKAhJDaGF0Ym90XE1vZHVsZXNcVjHiAh5DaGF0Ym90XE1vZHVsZXNcVjFcR1BCTWV0YWRhdGHqAhRDaGF0Ym90OjpNb2R1bGVzOjpWMWIGcHJvdG8z", [file_buf_validate_validate]);

/**
 * LlmConfig defines the configuration for the LLM module.
 * Controls model selection and inference parameters.
 *
 * @generated from message chatbot.modules.v1.LlmConfig
 */
export type LlmConfig = Message<"chatbot.modules.v1.LlmConfig"> & {
  /**
   * Whether the LLM module is enabled
   *
   * @generated from field: bool enabled = 1;
   */
  enabled: boolean;

  /**
   * SDK to use for LLM calls: "ai-sdk" (default)
   * Future: "langchain", "llamaindex", etc.
   *
   * @generated from field: string sdk = 2;
   */
  sdk: string;

  /**
   * AI provider to use: "bedrock" (default)
   * Future: "openai", "anthropic", "google", etc.
   *
   * @generated from field: string provider = 3;
   */
  provider: string;

  /**
   * Model identifier (e.g., us.anthropic.claude-sonnet-4-20250514-v1:0)
   *
   * @generated from field: string model = 4;
   */
  model: string;

  /**
   * Temperature for response randomness (0 = deterministic, 2 = very random)
   *
   * @generated from field: double temperature = 5;
   */
  temperature: number;

  /**
   * Maximum number of agent loop steps per conversation turn
   *
   * @generated from field: int32 max_steps = 6;
   */
  maxSteps: number;
};

/**
 * Describes the message chatbot.modules.v1.LlmConfig.
 * Use `create(LlmConfigSchema)` to create a new message.
 */
export const LlmConfigSchema: GenMessage<LlmConfig> = /*@__PURE__*/
  messageDesc(file_chatbot_modules_v1_llm, 0);

